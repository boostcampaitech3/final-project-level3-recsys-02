{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from konlpy.tag import *\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/final/data'\n",
    "food_path = os.path.join(data_dir, 'food.csv')\n",
    "USE_COLS = ['placeName', 'placeType', 'placeAddress', 'themeKeywords','like']\n",
    "raw_df = pd.read_csv(food_path, usecols=USE_COLS)\n",
    "raw_df = raw_df[~raw_df.placeType.str.contains('성급')].reset_index().copy()\n",
    "# raw_df = pd.read_json(data_path + 'placeInfo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['placeID'] = raw_df.apply(lambda x : x['placeName'] + x['placeAddress'], axis = 1)\n",
    "raw_df['placeID'] = raw_df['placeID'].apply(lambda x : x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>칼국수,만두</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>바(BAR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>양꼬치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>농부쌈밥서울동작구사당로30길19</td>\n",
       "      <td>쌈밥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티</td>\n",
       "      <td>김밥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           placeID feature\n",
       "0               밀밭정원서울마포구마포대로16길13  칼국수,만두\n",
       "1  식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)  바(BAR)\n",
       "2           동북양꼬치서울영등포구디지털로37길26-1     양꼬치\n",
       "3                농부쌈밥서울동작구사당로30길19      쌈밥\n",
       "4    홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티      김밥"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = raw_df[['placeID', 'placeType']]\n",
    "p_df.columns = ['placeID', 'feature']\n",
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_id(id_lst) :\n",
    "    id_lst.sort()\n",
    "    id_to_idx, idx_to_id = dict(), dict()\n",
    "    for index, value in enumerate(id_lst) :\n",
    "        id_to_idx[value] = index\n",
    "        idx_to_id[index] = value\n",
    "    return id_to_idx, idx_to_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### place theme keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hannanum = Hannanum()\n",
    "# komoran = Komoran()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_nouns(word:str):\n",
    "    # noun = hannanum.nouns(word)\n",
    "    # noun = komoran.nouns(word)\n",
    "    noun = okt.nouns(word)\n",
    "    if noun:\n",
    "        return noun[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2711/45279284.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  k_df['themeKeywords'] = k_df.themeKeywords.apply(eval)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>themeKeywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>[술집, 세계맥주, 맥주집, 호프집, 생맥주]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>농부쌈밥서울동작구사당로30길19</td>\n",
       "      <td>[인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           placeID  \\\n",
       "0               밀밭정원서울마포구마포대로16길13   \n",
       "1  식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)   \n",
       "2           동북양꼬치서울영등포구디지털로37길26-1   \n",
       "3                농부쌈밥서울동작구사당로30길19   \n",
       "4    홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티   \n",
       "\n",
       "                                       themeKeywords  \n",
       "0                                                 []  \n",
       "1                          [술집, 세계맥주, 맥주집, 호프집, 생맥주]  \n",
       "2                                                 []  \n",
       "3  [인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df = raw_df[['placeID', 'themeKeywords']]\n",
    "k_df['themeKeywords'] = k_df.themeKeywords.apply(eval)\n",
    "k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                [술집, 세계맥주, 맥주집, 호프집, 생맥주]\n",
       "3        [인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...\n",
       "6                                              [닭갈비, 닭갈비집]\n",
       "7                  [심플한, 돼지곱창, 시장, 소곱창, 곱창, 막창, 신선한, 숨어있는]\n",
       "8        [친절함, 친절하고, 화려한, 친절한, 시장, 소곱창, 양대창, 막창, 곱창, 나들...\n",
       "                               ...                        \n",
       "12646     [고급진, 깨끗한, 고급스러운, 안락한, 초밥, 젓갈, 튀김, 횟집, 참치회, 신선한]\n",
       "12651    [아늑한, 분위기좋은, 토속적인분위기, 김치찌개, 굴보쌈, 한정식, 곱창, 비빔밥,...\n",
       "12662                                [만두, 아이스크림, 설렁탕, 불고기]\n",
       "12664                         [닭갈비, 닭갈비집, 주먹밥, 막국수, 새로오픈한]\n",
       "12665    [고급진, 이국적, 고급스러운, 카레, 팟타이, 태국음식, 쌀국수, 누들, 나들이,...\n",
       "Name: themeKeywords, Length: 3499, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_place = k_df[k_df.themeKeywords.str.len()!=0]['themeKeywords']\n",
    "theme_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2711/2134242679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  k_df['prepThemeKeywords'] = k_df['themeKeywords'].apply(lambda x : list(map(lambda x : prep_nouns(x), x)))\n"
     ]
    }
   ],
   "source": [
    "k_df['prepThemeKeywords'] = k_df['themeKeywords'].apply(lambda x : list(map(lambda x : prep_nouns(x), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>themeKeywords</th>\n",
       "      <th>prepThemeKeywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>[술집, 세계맥주, 맥주집, 호프집, 생맥주]</td>\n",
       "      <td>[술집, 세계, 맥주, 호프, 생맥주]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>농부쌈밥서울동작구사당로30길19</td>\n",
       "      <td>[인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...</td>\n",
       "      <td>[인심, , , , 쌈밥, 제육, 오리, 부대찌개, 맛집, 부담, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12672</th>\n",
       "      <td>은빛바다광동수산행당동본점서울성동구행당로127-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12673</th>\n",
       "      <td>갓파스시분당미금역점경기성남시분당구돌마로67</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>갓잇송리단길점서울송파구백제고분로45길4-14</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12675</th>\n",
       "      <td>마니주호프수지점경기용인시수지구수지로342번길17에덴프라자</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>양재정육식당판교점경기성남시분당구분당내곡로1511층</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12677 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               placeID  \\\n",
       "0                   밀밭정원서울마포구마포대로16길13   \n",
       "1      식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)   \n",
       "2               동북양꼬치서울영등포구디지털로37길26-1   \n",
       "3                    농부쌈밥서울동작구사당로30길19   \n",
       "4        홍당무김밥서울영등포구문래로180영등포센트럴푸르지오시티   \n",
       "...                                ...   \n",
       "12672       은빛바다광동수산행당동본점서울성동구행당로127-1   \n",
       "12673          갓파스시분당미금역점경기성남시분당구돌마로67   \n",
       "12674         갓잇송리단길점서울송파구백제고분로45길4-14   \n",
       "12675  마니주호프수지점경기용인시수지구수지로342번길17에덴프라자   \n",
       "12676      양재정육식당판교점경기성남시분당구분당내곡로1511층   \n",
       "\n",
       "                                           themeKeywords  \\\n",
       "0                                                     []   \n",
       "1                              [술집, 세계맥주, 맥주집, 호프집, 생맥주]   \n",
       "2                                                     []   \n",
       "3      [인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...   \n",
       "4                                                     []   \n",
       "...                                                  ...   \n",
       "12672                                                 []   \n",
       "12673                                                 []   \n",
       "12674                                                 []   \n",
       "12675                                                 []   \n",
       "12676                                                 []   \n",
       "\n",
       "                            prepThemeKeywords  \n",
       "0                                          []  \n",
       "1                       [술집, 세계, 맥주, 호프, 생맥주]  \n",
       "2                                          []  \n",
       "3      [인심, , , , 쌈밥, 제육, 오리, 부대찌개, 맛집, 부담, ]  \n",
       "4                                          []  \n",
       "...                                       ...  \n",
       "12672                                      []  \n",
       "12673                                      []  \n",
       "12674                                      []  \n",
       "12675                                      []  \n",
       "12676                                      []  \n",
       "\n",
       "[12677 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '김피탕',\n",
       " '날',\n",
       " '콘서트',\n",
       " '초밥',\n",
       " '아바이순대',\n",
       " '미숫가루',\n",
       " '눈꽃',\n",
       " '메뉴',\n",
       " '제철',\n",
       " '초콜릿',\n",
       " '야식',\n",
       " '아메리칸스타일',\n",
       " '바게트',\n",
       " '해물파전',\n",
       " '떡볶이',\n",
       " '한치',\n",
       " '모던',\n",
       " '뚝방길',\n",
       " '닭볶음탕',\n",
       " '넉',\n",
       " '향토음식',\n",
       " '죽집',\n",
       " '김치찌개',\n",
       " '참치회',\n",
       " '오리',\n",
       " '디저트',\n",
       " '일식',\n",
       " '찹쌀떡',\n",
       " '냉면집',\n",
       " '당근',\n",
       " '크림',\n",
       " '똥',\n",
       " '라자냐',\n",
       " '도다리',\n",
       " '손',\n",
       " '앤',\n",
       " '선지국',\n",
       " '등',\n",
       " '누룽지',\n",
       " '스페인',\n",
       " '전통',\n",
       " '국수',\n",
       " '느낌',\n",
       " '킹크랩',\n",
       " '자연',\n",
       " '신선로',\n",
       " '카레',\n",
       " '밤',\n",
       " '로',\n",
       " '간짜장',\n",
       " '분위기',\n",
       " '치즈',\n",
       " '찹쌀',\n",
       " '팥죽',\n",
       " '쌈밥',\n",
       " '곱창전골',\n",
       " '알탕',\n",
       " '세계',\n",
       " '닭',\n",
       " '도루묵',\n",
       " '코스',\n",
       " '다슬기',\n",
       " '능이',\n",
       " '조각',\n",
       " '백숙',\n",
       " '건강',\n",
       " '보양식',\n",
       " '숙성',\n",
       " '웅장',\n",
       " '맛집',\n",
       " '바지락',\n",
       " '닭갈비',\n",
       " '닭꼬치',\n",
       " '연탄구이',\n",
       " '어복',\n",
       " '시래기',\n",
       " '방',\n",
       " '창',\n",
       " '게임방',\n",
       " '피맥',\n",
       " '굴국밥',\n",
       " '탕수육',\n",
       " '버섯',\n",
       " '생',\n",
       " '철길',\n",
       " '재첩',\n",
       " '초',\n",
       " '몽환',\n",
       " '광어',\n",
       " '완구',\n",
       " '훠궈',\n",
       " '모주',\n",
       " '게국지',\n",
       " '말',\n",
       " '밀크쉐이크',\n",
       " '산나물',\n",
       " '연탄',\n",
       " '뽈찜',\n",
       " '굴',\n",
       " '그리스',\n",
       " '집',\n",
       " '쫄면',\n",
       " '한우국밥',\n",
       " '정육',\n",
       " '초코',\n",
       " '퀘사디아',\n",
       " '복숭아',\n",
       " '블루베리',\n",
       " '아사',\n",
       " '초코파이',\n",
       " '명란',\n",
       " '회덮밥',\n",
       " '얼',\n",
       " '데이',\n",
       " '점심',\n",
       " '연어',\n",
       " '칵테일',\n",
       " '물',\n",
       " '고량주',\n",
       " '목살',\n",
       " '짱뚱어',\n",
       " '꼬',\n",
       " '닭강정',\n",
       " '오징어',\n",
       " '아울렛',\n",
       " '프라이',\n",
       " '참게',\n",
       " '멸치',\n",
       " '방영',\n",
       " '고기국수',\n",
       " '대게',\n",
       " '주물럭',\n",
       " '기념일',\n",
       " '커피',\n",
       " '민물장어',\n",
       " '해천',\n",
       " '붕어',\n",
       " '파르페',\n",
       " '녹차아이스크림',\n",
       " '메밀국수',\n",
       " '연잎밥',\n",
       " '어회',\n",
       " '떡갈비',\n",
       " '통차',\n",
       " '가정식',\n",
       " '단팥빵',\n",
       " '굴비',\n",
       " '설렁탕',\n",
       " '오믈렛',\n",
       " '저염식',\n",
       " '인절미',\n",
       " '찐빵',\n",
       " '비',\n",
       " '티라미수',\n",
       " '갈비',\n",
       " '식빵',\n",
       " '꼬마',\n",
       " '똥집',\n",
       " '편백나무',\n",
       " '김밥',\n",
       " '곱창',\n",
       " '곰치',\n",
       " '간장',\n",
       " '유니크',\n",
       " '옛날',\n",
       " '바나나',\n",
       " '아쿠아리움',\n",
       " '피순대',\n",
       " '딸기',\n",
       " '도토리묵',\n",
       " '조개',\n",
       " '얘기',\n",
       " '화이트데이',\n",
       " '추어탕',\n",
       " '시골',\n",
       " '닭집',\n",
       " '컵밥',\n",
       " '모찌',\n",
       " '채식',\n",
       " '러블리',\n",
       " '연어초밥',\n",
       " '한정식',\n",
       " '어구',\n",
       " '캐',\n",
       " '오분',\n",
       " '연말',\n",
       " '아나고',\n",
       " '샹그리',\n",
       " '산책',\n",
       " '만두국',\n",
       " '왕',\n",
       " '오므라이스',\n",
       " '멕시코',\n",
       " '칠리',\n",
       " '드라이브',\n",
       " '바질',\n",
       " '떡',\n",
       " '호프',\n",
       " '쭈꾸미',\n",
       " '촬영',\n",
       " '묵밥',\n",
       " '케밥',\n",
       " '고래',\n",
       " '인심',\n",
       " '밀크',\n",
       " '럭셔리',\n",
       " '깐풍기',\n",
       " '짜장면',\n",
       " '생맥주',\n",
       " '빈대떡',\n",
       " '김치볶음밥',\n",
       " '신비',\n",
       " '빨',\n",
       " '중국집',\n",
       " '퓨전',\n",
       " '파전',\n",
       " '방어',\n",
       " '소금',\n",
       " '빠',\n",
       " '박람회',\n",
       " '부담',\n",
       " '모밀',\n",
       " '인테리어',\n",
       " '녹차',\n",
       " '막국수',\n",
       " '장어구이',\n",
       " '폭포',\n",
       " '섭국',\n",
       " '밥',\n",
       " '합리',\n",
       " '재래시장',\n",
       " '발렌타인데이',\n",
       " '계곡',\n",
       " '석류',\n",
       " '돼지껍데기',\n",
       " '자리',\n",
       " '양장',\n",
       " '에그타르트',\n",
       " '연꽃',\n",
       " '똠양꿍',\n",
       " '완',\n",
       " '해물찜',\n",
       " '메기',\n",
       " '한식',\n",
       " '철판',\n",
       " '만두',\n",
       " '두루치기',\n",
       " '극장',\n",
       " '짬뽕',\n",
       " '양념',\n",
       " '고풍',\n",
       " '매운탕',\n",
       " '녹두',\n",
       " '파스타',\n",
       " '대나무',\n",
       " '고등',\n",
       " '토스트',\n",
       " '미로',\n",
       " '공연',\n",
       " '은공',\n",
       " '옻닭',\n",
       " '베이글',\n",
       " '비빔국수',\n",
       " '필라프',\n",
       " '칼제비',\n",
       " '도가니탕',\n",
       " '볼락',\n",
       " '물곰',\n",
       " '청국장',\n",
       " '카지노',\n",
       " '더덕',\n",
       " '양식',\n",
       " '봉골레',\n",
       " '런치',\n",
       " '일본',\n",
       " '카페',\n",
       " '공원',\n",
       " '보드카',\n",
       " '석갈비',\n",
       " '마늘',\n",
       " '크로켓',\n",
       " '비올',\n",
       " '리조또',\n",
       " '상추튀김',\n",
       " '자락',\n",
       " '케이블카',\n",
       " '불고기',\n",
       " '거북손',\n",
       " '서비스',\n",
       " '소갈비찜',\n",
       " '미니',\n",
       " '갯장어',\n",
       " '이동',\n",
       " '피자',\n",
       " '쌀국수',\n",
       " '족발',\n",
       " '내부',\n",
       " '복',\n",
       " '주스',\n",
       " '수다',\n",
       " '끼',\n",
       " '대하',\n",
       " '젤리',\n",
       " '메밀',\n",
       " '유니짜장',\n",
       " '이탈리안',\n",
       " '감자',\n",
       " '꽃게탕',\n",
       " '음식',\n",
       " '프레즐',\n",
       " '레스토랑',\n",
       " '나들이',\n",
       " '치즈케이크',\n",
       " '보쌈',\n",
       " '패밀리',\n",
       " '시장',\n",
       " '게찜',\n",
       " '생태',\n",
       " '냉',\n",
       " '유황오리',\n",
       " '대구',\n",
       " '양고기',\n",
       " '보리',\n",
       " '팬',\n",
       " '생선',\n",
       " '꽃게',\n",
       " '이상',\n",
       " '고추장',\n",
       " '규동',\n",
       " '마카롱',\n",
       " '매장',\n",
       " '엔틱',\n",
       " '결혼기념일',\n",
       " '브라우니',\n",
       " '야채',\n",
       " '도시락',\n",
       " '해물',\n",
       " '신혼여행',\n",
       " '오리고기',\n",
       " '유적',\n",
       " '가맥집',\n",
       " '만두전골',\n",
       " '운치',\n",
       " '분식',\n",
       " '해산물',\n",
       " '도리',\n",
       " '라운지',\n",
       " '규',\n",
       " '두부',\n",
       " '유럽',\n",
       " '맥',\n",
       " '기사',\n",
       " '젓갈',\n",
       " '미트볼',\n",
       " '삼계탕',\n",
       " '포장마차',\n",
       " '아트',\n",
       " '비빔밥',\n",
       " '우동',\n",
       " '야간',\n",
       " '숯불',\n",
       " '고르곤졸라',\n",
       " '아이스크림',\n",
       " '스타',\n",
       " '흑임자',\n",
       " '자극',\n",
       " '간',\n",
       " '비교',\n",
       " '핫',\n",
       " '명',\n",
       " '애플',\n",
       " '영화관',\n",
       " '돌',\n",
       " '팬케이크',\n",
       " '펍',\n",
       " '완탕',\n",
       " '바',\n",
       " '산낙지',\n",
       " '삼겹살',\n",
       " '드라마',\n",
       " '칼로리',\n",
       " '서점',\n",
       " '우유',\n",
       " '닭백숙',\n",
       " '재즈',\n",
       " '막걸리',\n",
       " '웰빙',\n",
       " '모듬전',\n",
       " '돼지갈비',\n",
       " '마약',\n",
       " '메론',\n",
       " '칼국수',\n",
       " '전복',\n",
       " '김치전',\n",
       " '봄꽃',\n",
       " '감성돔',\n",
       " '스쿠터',\n",
       " '홈',\n",
       " '팥빵',\n",
       " '꽃',\n",
       " '파불',\n",
       " '고추',\n",
       " '수수',\n",
       " '돈가스',\n",
       " '고기',\n",
       " '해변',\n",
       " '꼬리곰탕',\n",
       " '수제비',\n",
       " '꼬리',\n",
       " '다방',\n",
       " '감자탕',\n",
       " '수박',\n",
       " '물회',\n",
       " '떡국',\n",
       " '과일',\n",
       " '국',\n",
       " '모습',\n",
       " '팥빙수',\n",
       " '와인',\n",
       " '글',\n",
       " '해신',\n",
       " '퀄리티',\n",
       " '가격',\n",
       " '룸',\n",
       " '돔',\n",
       " '복어',\n",
       " '재첩국',\n",
       " '홍게',\n",
       " '선어회',\n",
       " '도서관',\n",
       " '청포도',\n",
       " '황태',\n",
       " '호수',\n",
       " '오픈',\n",
       " '개성',\n",
       " '뽈살',\n",
       " '연예인',\n",
       " '낭만',\n",
       " '다이닝',\n",
       " '츠케멘',\n",
       " '레드와인',\n",
       " '재',\n",
       " '전복죽',\n",
       " '와플',\n",
       " '햄버거',\n",
       " '냉채',\n",
       " '갈치',\n",
       " '인도',\n",
       " '박물관',\n",
       " '야시장',\n",
       " '낙지',\n",
       " '딤섬',\n",
       " '어묵',\n",
       " '프랑스',\n",
       " '철판요리',\n",
       " '백반',\n",
       " '국적',\n",
       " '리코',\n",
       " '과메기',\n",
       " '벤또',\n",
       " '돼지국밥',\n",
       " '골목',\n",
       " '민어',\n",
       " '오징어순대',\n",
       " '육개장',\n",
       " '전병',\n",
       " '수제',\n",
       " '수타면',\n",
       " '라면',\n",
       " '콩나물',\n",
       " '젓국',\n",
       " '콩국수',\n",
       " '한국',\n",
       " '격식',\n",
       " '캠핑',\n",
       " '멋',\n",
       " '샌드위치',\n",
       " '산행',\n",
       " '베이징',\n",
       " '치즈떡볶이',\n",
       " '곰탕',\n",
       " '회정',\n",
       " '가을',\n",
       " '왕새우',\n",
       " '식물원',\n",
       " '마늘빵',\n",
       " '베트남',\n",
       " '쟁반짜장',\n",
       " '추억',\n",
       " '꼼장어',\n",
       " '군',\n",
       " '한식당',\n",
       " '감각',\n",
       " '타코야끼',\n",
       " '함박스테이크',\n",
       " '이색',\n",
       " '참치',\n",
       " '브런치',\n",
       " '중독',\n",
       " '복국',\n",
       " '해물탕',\n",
       " '고급',\n",
       " '순두부찌개',\n",
       " '순대볶음',\n",
       " '직화',\n",
       " '북경오리',\n",
       " '카스테라',\n",
       " '횟집',\n",
       " '빈티',\n",
       " '아귀찜',\n",
       " '찜',\n",
       " '임실',\n",
       " '눈',\n",
       " '완당',\n",
       " '유물',\n",
       " '유린',\n",
       " '창코',\n",
       " '냉면',\n",
       " '옥돔',\n",
       " '아기자기',\n",
       " '꼬치',\n",
       " '흑우',\n",
       " '튀김',\n",
       " '케이크',\n",
       " '함박',\n",
       " '갈비탕',\n",
       " '프로포즈',\n",
       " '보리밥',\n",
       " '보신탕',\n",
       " '억새',\n",
       " '호두',\n",
       " '샐러드',\n",
       " '순두부',\n",
       " '육포',\n",
       " '동태',\n",
       " '갓김치',\n",
       " '휴식',\n",
       " '피크닉',\n",
       " '밀면',\n",
       " '단아',\n",
       " '납작만두',\n",
       " '닭칼국수',\n",
       " '선어',\n",
       " '개인',\n",
       " '수산시장',\n",
       " '치킨',\n",
       " '성게',\n",
       " '닭똥집',\n",
       " '등산',\n",
       " '스팸',\n",
       " '맥주',\n",
       " '소고기',\n",
       " '닭발',\n",
       " '중후',\n",
       " '국물',\n",
       " '치맥',\n",
       " '스테이크',\n",
       " '한우',\n",
       " '샤브샤브',\n",
       " '돼지',\n",
       " '육회',\n",
       " '빵집',\n",
       " '진흙',\n",
       " '데이트',\n",
       " '일식집',\n",
       " '경치',\n",
       " '통닭',\n",
       " '요즘',\n",
       " '기품',\n",
       " '탕',\n",
       " '멍게',\n",
       " '어죽',\n",
       " '찜닭',\n",
       " '자몽',\n",
       " '알밤',\n",
       " '망고',\n",
       " '우렁',\n",
       " '궁궐',\n",
       " '회전초밥',\n",
       " '심야식당',\n",
       " '부대찌개',\n",
       " '토속',\n",
       " '규모',\n",
       " '카르보나라',\n",
       " '미슐랭',\n",
       " '팟타이',\n",
       " '모듬회',\n",
       " '갈비찜',\n",
       " '심플',\n",
       " '통오징어',\n",
       " '피',\n",
       " '팥',\n",
       " '태국',\n",
       " '홍어',\n",
       " '새우',\n",
       " '게장',\n",
       " '석화',\n",
       " '볏짚',\n",
       " '술집',\n",
       " '도넛',\n",
       " '돌솥밥',\n",
       " '회',\n",
       " '독도',\n",
       " '국밥',\n",
       " '파니니',\n",
       " '문어',\n",
       " '간장게장',\n",
       " '꿀',\n",
       " '이자카야',\n",
       " '소갈비',\n",
       " '막창집',\n",
       " '샤',\n",
       " '동동주',\n",
       " '고등어',\n",
       " '감자전',\n",
       " '안동소주',\n",
       " '표고버섯',\n",
       " '힐링',\n",
       " '야경',\n",
       " '조개찜',\n",
       " '충무김밥',\n",
       " '막회',\n",
       " '사진',\n",
       " '막창',\n",
       " '순대',\n",
       " '만화카페',\n",
       " '민속',\n",
       " '화덕',\n",
       " '꼬막',\n",
       " '제육',\n",
       " '트렌디',\n",
       " '손님',\n",
       " '길거리',\n",
       " '누',\n",
       " '이태리',\n",
       " '도치',\n",
       " '장어',\n",
       " '갈매기살',\n",
       " '우회',\n",
       " '농장',\n",
       " '타르트',\n",
       " '유기농',\n",
       " '김치',\n",
       " '호두과자',\n",
       " '장작구이',\n",
       " '동그랑땡',\n",
       " '온천',\n",
       " '품격',\n",
       " '주먹밥',\n",
       " '하우스',\n",
       " '크레페',\n",
       " '파닭',\n",
       " '은']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_list = list(set(k_df[k_df.prepThemeKeywords.str.len()!=0]['prepThemeKeywords'].sum()))\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2711/3309127378.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p_df['placeID'] = p_df['placeID'].apply(lambda x: place_id2idx[f\"{x}\"])\n",
      "/tmp/ipykernel_2711/3309127378.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p_df['feature'] = p_df['feature'].apply(lambda x: feature_id2idx[f\"{x}\"])\n"
     ]
    }
   ],
   "source": [
    "place_id2idx, place_idx2id = remap_id(p_df['placeID'].unique())\n",
    "feature_id2idx, feature_idx2id = remap_id(list(set(list(p_df['feature']) + keyword_list)))\n",
    "\n",
    "p_df['placeID'] = p_df['placeID'].apply(lambda x: place_id2idx[f\"{x}\"])\n",
    "p_df['feature'] = p_df['feature'].apply(lambda x: feature_id2idx[f\"{x}\"])\n",
    "\n",
    "keyword_id2idx, keyword_idx2id = remap_id(list(set(keyword_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = pd.DataFrame([\n",
    "    [place_id2idx[id], keyword_id2idx[keyword]] for id, keywords in k_df[['placeID', 'prepThemeKeywords']].itertuples(index=False)\n",
    "    for keyword in keywords\n",
    "], columns=['placeID', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = k_df.groupby('feature').filter(lambda x : len(x)>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.concat([p_df, k_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df = pd.DataFrame.from_records(raw_df['like'].apply(eval))\n",
    "frequency = np.sum(~l_df.isna(), axis=0)\n",
    "like_cols = sorted(frequency[np.where(frequency > 1)[0]].index.values)\n",
    "like_to_idx, idx_to_like = remap_id(like_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df['placeID'] = raw_df['placeID'].apply(lambda x:place_id2idx[x])\n",
    "l_df.set_index('placeID', inplace=True)\n",
    "l_df.sort_index(inplace=True)\n",
    "l_df = l_df[like_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12677/12677 [00:18<00:00, 681.26it/s]\n"
     ]
    }
   ],
   "source": [
    "total_record = []\n",
    "for place_id in tqdm(l_df.index.values):\n",
    "    topk = np.argsort(l_df.fillna(0).values[place_id])[::-1]\n",
    "    cnt = 0\n",
    "    for t in topk:\n",
    "        if l_df.values[place_id, t] == 0 or cnt==5:\n",
    "            break\n",
    "        else :\n",
    "            total_record.append((place_id, t))\n",
    "            cnt += 1\n",
    "\n",
    "l_df = pd.DataFrame.from_records(total_record)\n",
    "l_df.columns = ['placeID', 'like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Heterogeneou Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consrtruct_graph(f_df, l_df) :\n",
    "    hg = dgl.heterograph({\n",
    "            ('place', 'pf', 'feature') : (list(f_df['placeID']), list(f_df['feature'])),\n",
    "            ('feature', 'fp', 'place') : (list(f_df['feature']), list(f_df['placeID'])),\n",
    "            ('place', 'pl', 'like') : (list(l_df['placeID']), list(l_df['like'])),\n",
    "            ('like', 'lp', 'place') : (list(l_df['like']), list(l_df['placeID'])),})\n",
    "    return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'feature': 745, 'like': 49, 'place': 12677},\n",
       "      num_edges={('feature', 'fp', 'place'): 39699, ('like', 'lp', 'place'): 57755, ('place', 'pf', 'feature'): 39699, ('place', 'pl', 'like'): 57755},\n",
       "      metagraph=[('feature', 'place', 'fp'), ('place', 'feature', 'pf'), ('place', 'like', 'pl'), ('like', 'place', 'lp')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg = consrtruct_graph(f_df, l_df)\n",
    "hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_walks_per_node = 5\n",
    "walk_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metapath(graph, place_idx2id) :\n",
    "    output_file = open(os.path.join('./', 'metapath.txt'), \"w\")\n",
    "    for p_idx in trange(graph.number_of_nodes('place')):\n",
    "        traces, _ = dgl.sampling.random_walk(\n",
    "            graph, [p_idx] * num_walks_per_node, metapath=['pf', 'fp', 'pl', 'lp'] * walk_length)\n",
    "\n",
    "        for tr in traces:\n",
    "            tr = tr[tr[:,]!=-1]\n",
    "            outline = ''\n",
    "            for i in range(len(tr)) :\n",
    "                # i % 2 == 1 을 통해 type도 포함해서 문장 생성 가능\n",
    "                if i % 2 == 0 :\n",
    "                    outline += place_idx2id[int(tr[i])] + ' '\n",
    "            print(outline, file= output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12677/12677 [00:10<00:00, 1208.50it/s]\n"
     ]
    }
   ],
   "source": [
    "create_metapath(hg, place_idx2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metapath2Vec Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장으로 만들어 저장한 metapath2vec.txt를 불러오는 과정\n",
    "class DataReader:\n",
    "    NEGATIVE_TABLE_SIZE = 1e8\n",
    "\n",
    "    def __init__(self, file_name, min_count, care_type):\n",
    "        self.negatives = []\n",
    "        self.discards = []\n",
    "        self.negpos = 0\n",
    "        self.care_type = care_type\n",
    "        self.word2id = dict() # 임베딩 생성할 단어와 학습과정에 사용할 인덱스\n",
    "        self.id2word = dict() # 임베딩 생성할 단어와 학습과정에 사용할 인덱스\n",
    "        self.sentences_count = 0\n",
    "        self.token_count = 0\n",
    "        self.word_frequency = dict()\n",
    "        self.inputFileName = file_name\n",
    "        self.read_words(min_count)\n",
    "        self.initTableNegatives()\n",
    "        self.initTableDiscards()\n",
    "\n",
    "    def read_words(self, min_count): \n",
    "        '''\n",
    "        텍스트 파일 읽으면서 각각 단어 등장 빈도 세기\n",
    "        '''\n",
    "        print(\"Read Words...\")\n",
    "        word_frequency = dict()\n",
    "        for line in open(self.inputFileName):\n",
    "            line = line.split()\n",
    "            if len(line) > 1:\n",
    "                self.sentences_count += 1\n",
    "                for word in line:\n",
    "                    if len(word) > 0:\n",
    "                        self.token_count += 1\n",
    "                        word_frequency[word] = word_frequency.get(word, 0) + 1 # get(key, default)\n",
    "\n",
    "                        if self.token_count % 1000000 == 0:\n",
    "                            print(\"Read \" + str(int(self.token_count / 1000000)) + \"M words.\")\n",
    "\n",
    "        wid = 0\n",
    "        for w, c in word_frequency.items(): # min_count 미만인 단어는 제외하고 단어 dictionary 생성\n",
    "            if c < min_count:\n",
    "                continue\n",
    "            self.word2id[w] = wid\n",
    "            self.id2word[wid] = w\n",
    "            self.word_frequency[wid] = c\n",
    "            wid += 1\n",
    "\n",
    "        self.word_count = len(self.word2id)\n",
    "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
    "\n",
    "    def initTableDiscards(self):\n",
    "        # get a frequency table for sub-sampling. Note that the frequency is adjusted by\n",
    "        # sub-sampling tricks.\n",
    "        t = 0.0001\n",
    "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
    "        self.discards = np.sqrt(t / f) + (t / f)\n",
    "\n",
    "    def initTableNegatives(self):\n",
    "        # get a table for negative sampling, if word with index 2 appears twice, then 2 will be listed\n",
    "        # in the table twice.\n",
    "        pow_frequency = np.array(list(self.word_frequency.values())) ** 0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
    "        for wid, c in enumerate(count):\n",
    "            self.negatives += [wid] * int(c)\n",
    "        self.negatives = np.array(self.negatives)\n",
    "        np.random.shuffle(self.negatives)\n",
    "        self.sampling_prob = ratio\n",
    "\n",
    "    def getNegatives(self, target, size):  # TODO check equality with target\n",
    "        if self.care_type == 0:\n",
    "            response = self.negatives[self.negpos:self.negpos + size]\n",
    "            self.negpos = (self.negpos + size) % len(self.negatives)\n",
    "            if len(response) != size:\n",
    "                return np.concatenate((response, self.negatives[0:self.negpos]))\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metapath2vec Dataset\n",
    "class Metapath2vecDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        # read in data, window_size and input filename\n",
    "        self.data = data\n",
    "        self.window_size = window_size # 타겟 단어 중심 몇 개의 단어를 볼 것인가\n",
    "        self.input_file = open(data.inputFileName)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of walks\n",
    "        return self.data.sentences_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the list of pairs (center, context, 5 negatives)\n",
    "        while True:\n",
    "            line = self.input_file.readline()\n",
    "            if not line:\n",
    "                self.input_file.seek(0, 0)\n",
    "                line = self.input_file.readline()\n",
    "\n",
    "            if len(line) > 1:\n",
    "                words = line.split()\n",
    "\n",
    "                if len(words) > 1:\n",
    "                    word_ids = [self.data.word2id[w] for w in words if\n",
    "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
    "\n",
    "                    pair_catch = []\n",
    "                    for i, u in enumerate(word_ids):\n",
    "                        for j, v in enumerate(\n",
    "                                word_ids[max(i - self.window_size, 0):i + self.window_size]):\n",
    "                            assert u < self.data.word_count\n",
    "                            assert v < self.data.word_count\n",
    "                            if i == j:\n",
    "                                continue\n",
    "                            pair_catch.append((u, v, self.data.getNegatives(v,5)))\n",
    "                    return pair_catch\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batches):\n",
    "        all_u = np.array([u for batch in batches for u, _, _ in batch if len(batch) > 0])\n",
    "        all_v = np.array([v for batch in batches for _, v, _ in batch if len(batch) > 0])\n",
    "        all_neg_v = np.array([neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0])\n",
    "\n",
    "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkipGram Model\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_dimension\n",
    "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
    "        init.constant_(self.v_embeddings.weight.data, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_neg_v = self.v_embeddings(neg_v)\n",
    "\n",
    "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metapath2vec \n",
    "class Metapath2VecTrainer:\n",
    "    def __init__(self, path):\n",
    "        min_count, care_type = 0, 0\n",
    "        batch_size, iterations = 50, 2\n",
    "        window_size, dim, initial_lr = 10, 128, 0.025\n",
    "        num_workers = 1\n",
    "        \n",
    "        self.data = DataReader(path, min_count, care_type)\n",
    "        dataset = Metapath2vecDataset(self.data, window_size)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                     shuffle=True, num_workers=num_workers, collate_fn=dataset.collate)\n",
    "        self.emb_size = len(self.data.word2id)\n",
    "        self.emb_dimension = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.initial_lr = initial_lr\n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        if self.use_cuda:\n",
    "            self.skip_gram_model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.SparseAdam(list(self.skip_gram_model.parameters()), lr=self.initial_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
    "\n",
    "        for iteration in range(self.iterations):\n",
    "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
    "            running_loss = 0.0\n",
    "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
    "                if len(sample_batched[0]) > 1:\n",
    "                    pos_u = sample_batched[0].to(self.device)\n",
    "                    pos_v = sample_batched[1].to(self.device)\n",
    "                    neg_v = sample_batched[2].to(self.device)\n",
    "\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
    "                    if i > 0 and i % 50000 == 0:\n",
    "                        print(\" Loss: \" + str(running_loss))\n",
    "        \n",
    "        self.skip_gram_model.save_embedding(self.data.id2word, data_dir+\"metapath_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Words...\n",
      "Total embeddings: 12677\n"
     ]
    }
   ],
   "source": [
    "m2v = Metapath2VecTrainer(os.path.join('./', \"metapath.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1268 [00:00<?, ?it/s]/tmp/ipykernel_2711/2793940150.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|██████████| 1268/1268 [00:25<00:00, 49.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1268 [00:00<?, ?it/s]/tmp/ipykernel_2711/2793940150.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)\n",
      "100%|██████████| 1268/1268 [00:25<00:00, 50.10it/s]\n"
     ]
    }
   ],
   "source": [
    "m2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/final/data'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_file() :\n",
    "    with open(data_dir + 'metapath_embeddings', 'r') as f:\n",
    "        id2word_len, emb_dimension = f.readline().split()\n",
    "        id2word = {}\n",
    "        word2id = {}\n",
    "        embeddings = []\n",
    "        idx = 0\n",
    "        while True :\n",
    "            z = f.readline()\n",
    "            if not z :\n",
    "                break\n",
    "            z = z.split()\n",
    "            word = z[0]\n",
    "            embedding = list(map(float, z[1:]))\n",
    "            embeddings.append(embedding)\n",
    "            id2word[idx] = word\n",
    "            word2id[word] = idx\n",
    "            idx += 1\n",
    "    return id2word, word2id, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2place, place2id, place_emb = create_embedding_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4061, 6291, 4688, 1975, 5686])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim = cosine_similarity(np.array(place_emb))\n",
    "topk = np.argsort(cossim[4061])[::-1][:5]\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('채선당샤브샤브&한우구이구로디지털점서울구로구디지털로32길30',\n",
       " '삼원일식서울서대문구통일로9안길322층',\n",
       " '이삭토스트신설동역점서울종로구종로393',\n",
       " '딸랏롯빠이서울관악구관악로12길113대원빌딩1층101호',\n",
       " '신천일호집서울송파구백제고분로7길28-13')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2place[6964], id2place[4061], id2place[8753], id2place[11357], id2place[3930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25848</th>\n",
       "      <td>5689</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25849</th>\n",
       "      <td>5689</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25850</th>\n",
       "      <td>5689</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25851</th>\n",
       "      <td>5689</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25852</th>\n",
       "      <td>5689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       placeID  like\n",
       "25848     5689    27\n",
       "25849     5689    39\n",
       "25850     5689    32\n",
       "25851     5689     5\n",
       "25852     5689     1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_df[l_df['placeID'] == place_id2idx[id2place[4061]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('생선회', '양이 많아요')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_idx2id[348], like_cols[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2): \n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2) \n",
    "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_list(df, place_id, feature):\n",
    "    return df[df.placeID == place_id][feature].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1 = get_feat_list(l_df, place_id2idx[id2place[4061]], 'like')\n",
    "feat2 = get_feat_list(l_df, place_id2idx[id2place[4688]], 'like')\n",
    "jaccard_similarity(feat1, feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_random(place_id, df, column):\n",
    "    m2v_topk = np.argsort(cossim[place_id])[::-1][1:11]\n",
    "    random_topk = np.array(random.sample(id2place.keys(), k = 10))\n",
    "    \n",
    "    target_feat_list = get_feat_list(df, place_id2idx[id2place[place_id]], column)\n",
    "    m2v_score, random_score = 0, 0\n",
    "    for pid in m2v_topk:\n",
    "        rec_feat_list = get_feat_list(df, place_id2idx[id2place[pid]], column)\n",
    "        m2v_score += jaccard_similarity(target_feat_list, rec_feat_list)\n",
    "    \n",
    "    for pid in random_topk:\n",
    "        rec_feat_list = get_feat_list(df, place_id2idx[id2place[pid]], column)\n",
    "        random_score += jaccard_similarity(target_feat_list, rec_feat_list)\n",
    "    \n",
    "    return m2v_score, random_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0909090909090908, 0.0)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_random(30, f_df, 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.476190476190476, 5.059523809523809)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_random(160, l_df, 'like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python ('lightgcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
