{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from konlpy.tag import *\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_json(data_path + 'placeInfo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['placeID'] = raw_df.apply(lambda x : x['placeName'] + x['placeAddress'], axis = 1)\n",
    "raw_df['placeID'] = raw_df['placeID'].apply(lambda x : x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>placeType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>청계다방서울서초구원터4길61층청계다방</td>\n",
       "      <td>카페,디저트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>칼국수,만두</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>투썸플레이스신정뉴타운점서울양천구신월로164</td>\n",
       "      <td>카페</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>바(BAR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>양꼬치</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           placeID placeType\n",
       "0             청계다방서울서초구원터4길61층청계다방    카페,디저트\n",
       "1               밀밭정원서울마포구마포대로16길13    칼국수,만두\n",
       "2          투썸플레이스신정뉴타운점서울양천구신월로164        카페\n",
       "3  식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)    바(BAR)\n",
       "4           동북양꼬치서울영등포구디지털로37길26-1       양꼬치"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df = raw_df[['placeID', 'placeType']]\n",
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_id(id_lst) :\n",
    "    id_lst.sort()\n",
    "    id_to_idx, idx_to_id = dict(), dict()\n",
    "    for index, value in enumerate(id_lst) :\n",
    "        id_to_idx[value] = index\n",
    "        idx_to_id[index] = value\n",
    "    return id_to_idx, idx_to_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_id2idx, place_idx2id = remap_id(p_df['placeID'].unique())\n",
    "type_id2idx, type_idx2id = remap_id(p_df['placeType'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29762/1217795841.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p_df['placeID'] = p_df['placeID'].apply(lambda x: place_id2idx[f\"{x}\"])\n",
      "/tmp/ipykernel_29762/1217795841.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p_df['placeType'] = p_df['placeType'].apply(lambda x: type_id2idx[f\"{x}\"])\n"
     ]
    }
   ],
   "source": [
    "p_df['placeID'] = p_df['placeID'].apply(lambda x: place_id2idx[f\"{x}\"])\n",
    "p_df['placeType'] = p_df['placeType'].apply(lambda x: type_id2idx[f\"{x}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>placeType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12775</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5449</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14390</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8609</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3316</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   placeID  placeType\n",
       "0    12775        231\n",
       "1     5449        232\n",
       "2    14390        230\n",
       "3     8609         96\n",
       "4     3316        162"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### place theme keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hannanum = Hannanum()\n",
    "# komoran = Komoran()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_nouns(word:str):\n",
    "    # noun = hannanum.nouns(word)\n",
    "    # noun = komoran.nouns(word)\n",
    "    noun = okt.nouns(word)\n",
    "    if noun:\n",
    "        return noun[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>themeKeywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>청계다방서울서초구원터4길61층청계다방</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>투썸플레이스신정뉴타운점서울양천구신월로164</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>[술집, 세계맥주, 맥주집, 호프집, 생맥주]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           placeID              themeKeywords\n",
       "0             청계다방서울서초구원터4길61층청계다방                         []\n",
       "1               밀밭정원서울마포구마포대로16길13                         []\n",
       "2          투썸플레이스신정뉴타운점서울양천구신월로164                         []\n",
       "3  식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)  [술집, 세계맥주, 맥주집, 호프집, 생맥주]\n",
       "4           동북양꼬치서울영등포구디지털로37길26-1                         []"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df = raw_df[['placeID', 'themeKeywords']]\n",
    "k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3                                [술집, 세계맥주, 맥주집, 호프집, 생맥주]\n",
       "5        [인심좋은, 친절한, 친절하신, 친절하고, 쌈밥, 제육볶음, 오리로스, 부대찌개, ...\n",
       "8                                              [닭갈비, 닭갈비집]\n",
       "10                 [심플한, 돼지곱창, 시장, 소곱창, 곱창, 막창, 신선한, 숨어있는]\n",
       "11       [친절함, 친절하고, 화려한, 친절한, 시장, 소곱창, 양대창, 막창, 곱창, 나들...\n",
       "                               ...                        \n",
       "16314    [아늑한, 빈티지한, 분위기있는, 수제초콜릿, 초코빙수, 마카롱, 팬케이크, 발렌타...\n",
       "16315                                [만두, 아이스크림, 설렁탕, 불고기]\n",
       "16317                         [닭갈비, 닭갈비집, 주먹밥, 막국수, 새로오픈한]\n",
       "16318    [고급진, 이국적, 고급스러운, 카레, 팟타이, 태국음식, 쌀국수, 누들, 나들이,...\n",
       "16330                                         [아이스크림, 케이크]\n",
       "Name: themeKeywords, Length: 4017, dtype: object"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_place = k_df[k_df.themeKeywords.str.len()!=0]['themeKeywords']\n",
    "theme_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29762/2134242679.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  k_df['prepThemeKeywords'] = k_df['themeKeywords'].apply(lambda x : list(map(lambda x : prep_nouns(x), x)))\n"
     ]
    }
   ],
   "source": [
    "k_df['prepThemeKeywords'] = k_df['themeKeywords'].apply(lambda x : list(map(lambda x : prep_nouns(x), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeID</th>\n",
       "      <th>themeKeywords</th>\n",
       "      <th>prepThemeKeywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>청계다방서울서초구원터4길61층청계다방</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>밀밭정원서울마포구마포대로16길13</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>투썸플레이스신정뉴타운점서울양천구신월로164</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)</td>\n",
       "      <td>[술집, 세계맥주, 맥주집, 호프집, 생맥주]</td>\n",
       "      <td>[술집, 세계, 맥주, 호프, 생맥주]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>동북양꼬치서울영등포구디지털로37길26-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16326</th>\n",
       "      <td>갓파스시분당미금역점경기성남시분당구돌마로67</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16327</th>\n",
       "      <td>갓잇송리단길점서울송파구백제고분로45길4-14</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16328</th>\n",
       "      <td>마니주호프수지점경기용인시수지구수지로342번길17에덴프라자</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16329</th>\n",
       "      <td>양재정육식당판교점경기성남시분당구분당내곡로1511층</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16330</th>\n",
       "      <td>배스킨라빈스구로구청점서울구로구구로중앙로68(구로동,신안타워)1층102-1호(구로동,...</td>\n",
       "      <td>[아이스크림, 케이크]</td>\n",
       "      <td>[아이스크림, 케이크]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 placeID  \\\n",
       "0                                   청계다방서울서초구원터4길61층청계다방   \n",
       "1                                     밀밭정원서울마포구마포대로16길13   \n",
       "2                                투썸플레이스신정뉴타운점서울양천구신월로164   \n",
       "3                        식스센스다이닝BAR서울동대문구왕산로2길9(2층방역룸예약)   \n",
       "4                                 동북양꼬치서울영등포구디지털로37길26-1   \n",
       "...                                                  ...   \n",
       "16326                            갓파스시분당미금역점경기성남시분당구돌마로67   \n",
       "16327                           갓잇송리단길점서울송파구백제고분로45길4-14   \n",
       "16328                    마니주호프수지점경기용인시수지구수지로342번길17에덴프라자   \n",
       "16329                        양재정육식당판교점경기성남시분당구분당내곡로1511층   \n",
       "16330  배스킨라빈스구로구청점서울구로구구로중앙로68(구로동,신안타워)1층102-1호(구로동,...   \n",
       "\n",
       "                   themeKeywords      prepThemeKeywords  \n",
       "0                             []                     []  \n",
       "1                             []                     []  \n",
       "2                             []                     []  \n",
       "3      [술집, 세계맥주, 맥주집, 호프집, 생맥주]  [술집, 세계, 맥주, 호프, 생맥주]  \n",
       "4                             []                     []  \n",
       "...                          ...                    ...  \n",
       "16326                         []                     []  \n",
       "16327                         []                     []  \n",
       "16328                         []                     []  \n",
       "16329                         []                     []  \n",
       "16330               [아이스크림, 케이크]           [아이스크림, 케이크]  \n",
       "\n",
       "[16331 rows x 3 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '신비',\n",
       " '치즈케이크',\n",
       " '고급',\n",
       " '청포도',\n",
       " '대구',\n",
       " '컵케이크',\n",
       " '우동',\n",
       " '돼지국밥',\n",
       " '팬',\n",
       " '아사',\n",
       " '한우국밥',\n",
       " '보쌈',\n",
       " '냉',\n",
       " '치킨',\n",
       " '부대찌개',\n",
       " '스타',\n",
       " '퀄리티',\n",
       " '손님',\n",
       " '프로포즈',\n",
       " '다방',\n",
       " '고기국수',\n",
       " '생',\n",
       " '창',\n",
       " '회덮밥',\n",
       " '복어',\n",
       " '청국장',\n",
       " '홍게',\n",
       " '하우스',\n",
       " '제철',\n",
       " '해신',\n",
       " '똥집',\n",
       " '고등',\n",
       " '빵',\n",
       " '빨',\n",
       " '화이트데이',\n",
       " '레스토랑',\n",
       " '거북손',\n",
       " '유니짜장',\n",
       " '카레',\n",
       " '드립커피',\n",
       " '꼬치',\n",
       " '식물원',\n",
       " '눈꽃',\n",
       " '느낌',\n",
       " '크레이프',\n",
       " '녹차아이스크림',\n",
       " '모듬회',\n",
       " '이동',\n",
       " '깐풍기',\n",
       " '가족',\n",
       " '숙성',\n",
       " '누룽지',\n",
       " '문어',\n",
       " '일본',\n",
       " '해변',\n",
       " '글',\n",
       " '수타면',\n",
       " '옥돔',\n",
       " '갤러리',\n",
       " '젓갈',\n",
       " '다이닝',\n",
       " '음식',\n",
       " '참치회',\n",
       " '옛날',\n",
       " '곱창전골',\n",
       " '온천',\n",
       " '얘기',\n",
       " '볼락',\n",
       " '순대',\n",
       " '츠케멘',\n",
       " '포장마차',\n",
       " '닭갈비',\n",
       " '팥빵',\n",
       " '개인',\n",
       " '동그랑땡',\n",
       " '고량주',\n",
       " '갈대',\n",
       " '일식',\n",
       " '아트',\n",
       " '해물',\n",
       " '퀘사디아',\n",
       " '테디베어',\n",
       " '와인',\n",
       " '닭꼬치',\n",
       " '은',\n",
       " '철판요리',\n",
       " '연탄',\n",
       " '감자',\n",
       " '스팸',\n",
       " '수영장',\n",
       " '간장게장',\n",
       " '뽈살',\n",
       " '맘모스',\n",
       " '칼국수',\n",
       " '제육',\n",
       " '모듬전',\n",
       " '라자냐',\n",
       " '딸기',\n",
       " '한라봉',\n",
       " '삼계탕',\n",
       " '북경오리',\n",
       " '수제',\n",
       " '메밀국수',\n",
       " '돈가스',\n",
       " '보양식',\n",
       " '이상',\n",
       " '수제비',\n",
       " '마카롱',\n",
       " '양고기',\n",
       " '딸기모찌',\n",
       " '부담',\n",
       " '세계',\n",
       " '도루묵',\n",
       " '콩국수',\n",
       " '필라프',\n",
       " '칠리',\n",
       " '자몽',\n",
       " '왕새우',\n",
       " '동동주',\n",
       " '육포',\n",
       " '멋',\n",
       " '호두과자',\n",
       " '매운탕',\n",
       " '우회',\n",
       " '석류',\n",
       " '빈대떡',\n",
       " '코스',\n",
       " '석갈비',\n",
       " '국물',\n",
       " '추억',\n",
       " '솜사탕',\n",
       " '성곽',\n",
       " '전병',\n",
       " '박물관',\n",
       " '면세점',\n",
       " '한국',\n",
       " '운치',\n",
       " '대화',\n",
       " '빵집',\n",
       " '샐러드',\n",
       " '완탕',\n",
       " '벤또',\n",
       " '호수',\n",
       " '굴비',\n",
       " '대나무',\n",
       " '붕어빵',\n",
       " '어구',\n",
       " '케이크',\n",
       " '토속',\n",
       " '설렁탕',\n",
       " '티크',\n",
       " '보리밥',\n",
       " '수박',\n",
       " '튀김',\n",
       " '장어',\n",
       " '분위기',\n",
       " '흑임자',\n",
       " '목살',\n",
       " '막걸리',\n",
       " '등산',\n",
       " '결혼기념일',\n",
       " '앤',\n",
       " '촬영',\n",
       " '내부',\n",
       " '함박',\n",
       " '드라마',\n",
       " '베이징',\n",
       " '벽화',\n",
       " '스테이크',\n",
       " '억새',\n",
       " '피맥',\n",
       " '연말',\n",
       " '뽈찜',\n",
       " '상추튀김',\n",
       " '휴식',\n",
       " '바지락',\n",
       " '떡갈비',\n",
       " '돼지껍데기',\n",
       " '드라이브',\n",
       " '연어',\n",
       " '등',\n",
       " '요즘',\n",
       " '백반',\n",
       " '크로켓',\n",
       " '회정',\n",
       " '중국집',\n",
       " '방영',\n",
       " '연극',\n",
       " '조개',\n",
       " '메기',\n",
       " '장작구이',\n",
       " '명',\n",
       " '막창집',\n",
       " '선어회',\n",
       " '야경',\n",
       " '연잎밥',\n",
       " '팥죽',\n",
       " '규동',\n",
       " '표고버섯',\n",
       " '홈',\n",
       " '해물탕',\n",
       " '생선',\n",
       " '수수',\n",
       " '비빔국수',\n",
       " '태국',\n",
       " '눈',\n",
       " '동물',\n",
       " '독도',\n",
       " '도가니탕',\n",
       " '광어',\n",
       " '창코',\n",
       " '간짜장',\n",
       " '더덕',\n",
       " '돼지갈비',\n",
       " '연탄구이',\n",
       " '완',\n",
       " '녹차',\n",
       " '컵밥',\n",
       " '통닭',\n",
       " '고추장',\n",
       " '목장',\n",
       " '햄버거',\n",
       " '카페',\n",
       " '서점',\n",
       " '초밥',\n",
       " '김치',\n",
       " '와플',\n",
       " '완구',\n",
       " '멕시코',\n",
       " '멜론',\n",
       " '빈티',\n",
       " '럭셔리',\n",
       " '메밀',\n",
       " '샌드위치',\n",
       " '도넛',\n",
       " '닭볶음탕',\n",
       " '허니문',\n",
       " '발렌타인데이',\n",
       " '추어탕',\n",
       " '곰탕',\n",
       " '도치',\n",
       " '미술관',\n",
       " '불고기',\n",
       " '굴국밥',\n",
       " '이탈리안',\n",
       " '만화카페',\n",
       " '비교',\n",
       " '메론',\n",
       " '꼬',\n",
       " '야채',\n",
       " '어죽',\n",
       " '옻닭',\n",
       " '낭만',\n",
       " '도서관',\n",
       " '김피탕',\n",
       " '황태',\n",
       " '카스테라',\n",
       " '탕',\n",
       " '젤리',\n",
       " '런치',\n",
       " '국적',\n",
       " '명란',\n",
       " '유황',\n",
       " '오픈',\n",
       " '유적',\n",
       " '일식집',\n",
       " '케밥',\n",
       " '웰빙',\n",
       " '우렁',\n",
       " '나들이',\n",
       " '과메기',\n",
       " '재래시장',\n",
       " '향토음식',\n",
       " '오징어순대',\n",
       " '막국수',\n",
       " '육개장',\n",
       " '갯장어',\n",
       " '중독',\n",
       " '묵밥',\n",
       " '말',\n",
       " '함박스테이크',\n",
       " '피',\n",
       " '쌀국수',\n",
       " '비올',\n",
       " '베트남',\n",
       " '자리',\n",
       " '엔틱',\n",
       " '이색',\n",
       " '주스',\n",
       " '양장',\n",
       " '고추',\n",
       " '어회',\n",
       " '똠양꿍',\n",
       " '소금',\n",
       " '치맥',\n",
       " '스콘',\n",
       " '빠',\n",
       " '꼬리',\n",
       " '수다',\n",
       " '버섯',\n",
       " '유럽',\n",
       " '알밤',\n",
       " '횟집',\n",
       " '전복',\n",
       " '미트볼',\n",
       " '옥',\n",
       " '뚝방길',\n",
       " '감성돔',\n",
       " '민속',\n",
       " '복국',\n",
       " '소고기',\n",
       " '재',\n",
       " '충무김밥',\n",
       " '밤',\n",
       " '한식당',\n",
       " '마늘',\n",
       " '프랑스',\n",
       " '작업',\n",
       " '에그타르트',\n",
       " '육회',\n",
       " '고르곤졸라',\n",
       " '어복',\n",
       " '젓국',\n",
       " '찐빵',\n",
       " '유러',\n",
       " '참게',\n",
       " '중후',\n",
       " '개성',\n",
       " '슈크림',\n",
       " '선지국',\n",
       " '고래',\n",
       " '군',\n",
       " '기념일',\n",
       " '회',\n",
       " '프라이',\n",
       " '완당',\n",
       " '양념',\n",
       " '스쿠터',\n",
       " '백숙',\n",
       " '쟁반짜장',\n",
       " '영화관',\n",
       " '짱뚱어',\n",
       " '과일',\n",
       " '닭똥집',\n",
       " '케이블카',\n",
       " '꼼장어',\n",
       " '떡국',\n",
       " '애플',\n",
       " '한치',\n",
       " '알탕',\n",
       " '매장',\n",
       " '밀크쉐이크',\n",
       " '퓨전',\n",
       " '재첩국',\n",
       " '꽃게탕',\n",
       " '방어',\n",
       " '콩나물',\n",
       " '닭칼국수',\n",
       " '갈매기살',\n",
       " '야식',\n",
       " '생맥주',\n",
       " '보드카',\n",
       " '대게',\n",
       " '폭포',\n",
       " '극장',\n",
       " '핫',\n",
       " '도지',\n",
       " '갓김치',\n",
       " '캠핑',\n",
       " '모던',\n",
       " '아이스크림',\n",
       " '분식',\n",
       " '카지노',\n",
       " '밀면',\n",
       " '조각',\n",
       " '도다리',\n",
       " '가정식',\n",
       " '데이트',\n",
       " '홍어',\n",
       " '복숭아',\n",
       " '곱창',\n",
       " '토스트',\n",
       " '납작만두',\n",
       " '편백나무',\n",
       " '식빵',\n",
       " '게국지',\n",
       " '아울렛',\n",
       " '계곡',\n",
       " '합리',\n",
       " '연꽃',\n",
       " '샤',\n",
       " '오징어',\n",
       " '봉골레',\n",
       " '칼로리',\n",
       " '단팥빵',\n",
       " '해산물',\n",
       " '쌀빵',\n",
       " '점심',\n",
       " '팥빙수',\n",
       " '박람회',\n",
       " '인테리어',\n",
       " '규모',\n",
       " '얼',\n",
       " '오레오',\n",
       " '킹크랩',\n",
       " '장어구이',\n",
       " '카트',\n",
       " '쇼파',\n",
       " '짜장면',\n",
       " '만두전골',\n",
       " '한우',\n",
       " '밀크',\n",
       " '야시장',\n",
       " '호두',\n",
       " '소갈비찜',\n",
       " '낙지',\n",
       " '성게',\n",
       " '맥',\n",
       " '서비스',\n",
       " '공연',\n",
       " '공원',\n",
       " '민어',\n",
       " '화과자',\n",
       " '이자카야',\n",
       " '건강',\n",
       " '김치볶음밥',\n",
       " '꿀',\n",
       " '자락',\n",
       " '한정식',\n",
       " '티라미수',\n",
       " '피크닉',\n",
       " '파닭',\n",
       " '연예인',\n",
       " '로스팅',\n",
       " '베이글',\n",
       " '왕',\n",
       " '국',\n",
       " '로',\n",
       " '그리스',\n",
       " '수산시장',\n",
       " '리코',\n",
       " '멍게',\n",
       " '파스타',\n",
       " '몽환',\n",
       " '돼지',\n",
       " '유물',\n",
       " '순두부',\n",
       " '격식',\n",
       " '놀이동산',\n",
       " '훠궈',\n",
       " '트렌디',\n",
       " '동굴',\n",
       " '붕어',\n",
       " '심야식당',\n",
       " '라면',\n",
       " '키즈',\n",
       " '웅장',\n",
       " '시래기',\n",
       " '찜',\n",
       " '찹쌀떡',\n",
       " '양식',\n",
       " '돔',\n",
       " '캐',\n",
       " '석화',\n",
       " '흑우',\n",
       " '스페인',\n",
       " '유황오리',\n",
       " '타르트',\n",
       " '안동소주',\n",
       " '떡볶이',\n",
       " '치즈떡볶이',\n",
       " '꽃게',\n",
       " '가맥집',\n",
       " '전복죽',\n",
       " '도리',\n",
       " '탕수육',\n",
       " '다슬기',\n",
       " '데이',\n",
       " '기사',\n",
       " '오분',\n",
       " '참치',\n",
       " '해물파전',\n",
       " '모습',\n",
       " '딤섬',\n",
       " '품격',\n",
       " '피순대',\n",
       " '냉면',\n",
       " '마늘빵',\n",
       " '두루치기',\n",
       " '체험',\n",
       " '재첩',\n",
       " '룸',\n",
       " '꼬막',\n",
       " '산낙지',\n",
       " '죽집',\n",
       " '청보리',\n",
       " '생태',\n",
       " '젤라또',\n",
       " '오믈렛',\n",
       " '시장',\n",
       " '철판',\n",
       " '자극',\n",
       " '망고',\n",
       " '막회',\n",
       " '브라우니',\n",
       " '신선로',\n",
       " '재즈',\n",
       " '레드와인',\n",
       " '마약',\n",
       " '샹그리',\n",
       " '짬뽕',\n",
       " '롤케이크',\n",
       " '카르보나라',\n",
       " '오므라이스',\n",
       " '라운지',\n",
       " '순두부찌개',\n",
       " '숯불',\n",
       " '도시락',\n",
       " '츄러스',\n",
       " '직화',\n",
       " '호프',\n",
       " '무지개',\n",
       " '게장',\n",
       " '초콜릿',\n",
       " '갈비',\n",
       " '유린',\n",
       " '전통',\n",
       " '사진',\n",
       " '기품',\n",
       " '방',\n",
       " '그린티',\n",
       " '임실',\n",
       " '찻집',\n",
       " '굴',\n",
       " '만두',\n",
       " '냉채',\n",
       " '소갈비',\n",
       " '아바이순대',\n",
       " '바나나',\n",
       " '닭집',\n",
       " '바게트',\n",
       " '메뉴',\n",
       " '찜닭',\n",
       " '규',\n",
       " '초',\n",
       " '능이',\n",
       " '동태',\n",
       " '비빔밥',\n",
       " '복',\n",
       " '커피',\n",
       " '도쿄',\n",
       " '디저트',\n",
       " '산책',\n",
       " '바질',\n",
       " '파르페',\n",
       " '팟타이',\n",
       " '물곰',\n",
       " '피자',\n",
       " '우유',\n",
       " '녹두',\n",
       " '물',\n",
       " '미숫가루',\n",
       " '어묵',\n",
       " '인절미',\n",
       " '조개찜',\n",
       " '심플',\n",
       " '라이브',\n",
       " '모찌',\n",
       " '정육',\n",
       " '타로',\n",
       " '호떡',\n",
       " '모밀',\n",
       " '족발',\n",
       " '꽃',\n",
       " '궁궐',\n",
       " '맥주',\n",
       " '김밥',\n",
       " '쭈꾸미',\n",
       " '저염식',\n",
       " '아메리칸스타일',\n",
       " '연어초밥',\n",
       " '갈비찜',\n",
       " '선어',\n",
       " '봄꽃',\n",
       " '쫄면',\n",
       " '치즈',\n",
       " '곰치',\n",
       " '가을',\n",
       " '햇살',\n",
       " '날',\n",
       " '떡',\n",
       " '물회',\n",
       " '산나물',\n",
       " '끼',\n",
       " '닭발',\n",
       " '섭국',\n",
       " '보리',\n",
       " '서커스',\n",
       " '미슐랭',\n",
       " '돌',\n",
       " '초코파이',\n",
       " '고등어',\n",
       " '주물럭',\n",
       " '돌솥밥',\n",
       " '통차',\n",
       " '갈비탕',\n",
       " '미로',\n",
       " '감각',\n",
       " '막창',\n",
       " '술집',\n",
       " '순대볶음',\n",
       " '꼬리곰탕',\n",
       " '야간',\n",
       " '파불',\n",
       " '산행',\n",
       " '꽈배기',\n",
       " '밥',\n",
       " '꼬마',\n",
       " '비',\n",
       " '팬케이크',\n",
       " '해천',\n",
       " '만두국',\n",
       " '손',\n",
       " '인심',\n",
       " '김치찌개',\n",
       " '뮤지컬',\n",
       " '이태리',\n",
       " '인도',\n",
       " '길거리',\n",
       " '닭',\n",
       " '펍',\n",
       " '좌',\n",
       " '칼제비',\n",
       " '김치전',\n",
       " '모주',\n",
       " '자연',\n",
       " '아기자기',\n",
       " '게찜',\n",
       " '국수',\n",
       " '맛집',\n",
       " '통오징어',\n",
       " '채식',\n",
       " '힐링',\n",
       " '유니크',\n",
       " '신혼여행',\n",
       " '시골',\n",
       " '갈치',\n",
       " '철길',\n",
       " '해물찜',\n",
       " '넉',\n",
       " '민물장어',\n",
       " '아쿠아리움',\n",
       " '골목',\n",
       " '농장',\n",
       " '새우',\n",
       " '아귀찜',\n",
       " '멸치',\n",
       " '크림',\n",
       " '파전',\n",
       " '유기농',\n",
       " '삼겹살',\n",
       " '칵테일',\n",
       " '제과점',\n",
       " '크레페',\n",
       " '프레즐',\n",
       " '은공',\n",
       " '초코',\n",
       " '리조또',\n",
       " '아나고',\n",
       " '국밥',\n",
       " '보신탕',\n",
       " '미니',\n",
       " '프리',\n",
       " '브런치',\n",
       " '영화',\n",
       " '감자탕',\n",
       " '팥',\n",
       " '도토리묵',\n",
       " '닭백숙',\n",
       " '주먹밥',\n",
       " '당근',\n",
       " '게임방',\n",
       " '단아',\n",
       " '샤브샤브',\n",
       " '패밀리',\n",
       " '콘서트',\n",
       " '두부',\n",
       " '똥',\n",
       " '간장',\n",
       " '쌈밥',\n",
       " '진흙',\n",
       " '볏짚',\n",
       " '냉면집',\n",
       " '대하',\n",
       " '화덕',\n",
       " '파니니',\n",
       " '가격',\n",
       " '오리',\n",
       " '한식',\n",
       " '핸드',\n",
       " '찹쌀',\n",
       " '간',\n",
       " '생초콜릿',\n",
       " '블루베리',\n",
       " '고풍',\n",
       " '고기',\n",
       " '감자전',\n",
       " '집',\n",
       " '경치',\n",
       " '러블리',\n",
       " '닭강정',\n",
       " '바',\n",
       " '회전초밥',\n",
       " '누',\n",
       " '오리고기',\n",
       " '타코야끼']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_list = list(set(k_df[k_df.prepThemeKeywords.str.len()!=0]['prepThemeKeywords'].sum()))\n",
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_id2idx, keyword_idx2id = remap_id(list(set(keyword_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = pd.DataFrame([\n",
    "    [place_id2idx[id], keyword_id2idx[keyword]] for id, keywords in k_df[['placeID', 'prepThemeKeywords']].itertuples(index=False)\n",
    "    for keyword in keywords\n",
    "], columns=['placeID', 'prepThemeKeywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = k_df.groupby('prepThemeKeywords').filter(lambda x : len(x)>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Heterogeneou Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consrtruct_graph(df, k_df) :\n",
    "    hg = dgl.heterograph({\n",
    "            ('place', 'pt', 'type') : (list(df['placeID']), list(df['placeType'])),\n",
    "            ('type', 'tp', 'place') : (list(df['placeType']), list(df['placeID'])),\n",
    "            ('place', 'pk', 'keyword') : (list(k_df['placeID']), list(k_df['prepThemeKeywords'])),\n",
    "            ('keyword', 'kp', 'place') : (list(k_df['prepThemeKeywords']), list(k_df['placeID']))})\n",
    "    return hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'keyword': 717, 'place': 16331, 'type': 271},\n",
       "      num_edges={('keyword', 'kp', 'place'): 30925, ('place', 'pk', 'keyword'): 30925, ('place', 'pt', 'type'): 16331, ('type', 'tp', 'place'): 16331},\n",
       "      metagraph=[('keyword', 'place', 'kp'), ('place', 'keyword', 'pk'), ('place', 'type', 'pt'), ('type', 'place', 'tp')])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg = consrtruct_graph(p_df, k_df)\n",
    "hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_walks_per_node = 5\n",
    "walk_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metapath(graph, place_idx2id) :\n",
    "    output_file = open(os.path.join(data_path, 'metapath.txt'), \"w\")\n",
    "    for p_idx in trange(graph.number_of_nodes('place')):\n",
    "        traces, _ = dgl.sampling.random_walk(\n",
    "            graph, [p_idx] * num_walks_per_node, metapath=['pt', 'tp'] * walk_length)\n",
    "\n",
    "\n",
    "        for tr in traces:\n",
    "            tr = tr[tr[:,]!=-1]\n",
    "            outline = ''\n",
    "            for i in range(len(tr)) :\n",
    "                # i % 2 == 1 을 통해 type도 포함해서 문장 생성 가능\n",
    "                if i % 2 == 0 :\n",
    "                    outline += place_idx2id[int(tr[i])] + ' '\n",
    "            print(outline, file= output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11437/11437 [00:07<00:00, 1603.68it/s]\n"
     ]
    }
   ],
   "source": [
    "create_metapath(hg, place_idx2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metapath2Vec Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장으로 만들어 저장한 metapath2vec.txt를 불러오는 과정\n",
    "class DataReader:\n",
    "    NEGATIVE_TABLE_SIZE = 1e8\n",
    "\n",
    "    def __init__(self, file_name, min_count, care_type):\n",
    "        self.negatives = []\n",
    "        self.discards = []\n",
    "        self.negpos = 0\n",
    "        self.care_type = care_type\n",
    "        self.word2id = dict() # 임베딩 생성할 단어와 학습과정에 사용할 인덱스\n",
    "        self.id2word = dict() # 임베딩 생성할 단어와 학습과정에 사용할 인덱스\n",
    "        self.sentences_count = 0\n",
    "        self.token_count = 0\n",
    "        self.word_frequency = dict()\n",
    "        self.inputFileName = file_name\n",
    "        self.read_words(min_count)\n",
    "        self.initTableNegatives()\n",
    "        self.initTableDiscards()\n",
    "\n",
    "    def read_words(self, min_count): \n",
    "        '''\n",
    "        텍스트 파일 읽으면서 각각 단어 등장 빈도 세기\n",
    "        '''\n",
    "        print(\"Read Words...\")\n",
    "        word_frequency = dict()\n",
    "        for line in open(self.inputFileName):\n",
    "            line = line.split()\n",
    "            if len(line) > 1:\n",
    "                self.sentences_count += 1\n",
    "                for word in line:\n",
    "                    if len(word) > 0:\n",
    "                        self.token_count += 1\n",
    "                        word_frequency[word] = word_frequency.get(word, 0) + 1 # get(key, default)\n",
    "\n",
    "                        if self.token_count % 1000000 == 0:\n",
    "                            print(\"Read \" + str(int(self.token_count / 1000000)) + \"M words.\")\n",
    "\n",
    "        wid = 0\n",
    "        for w, c in word_frequency.items(): # min_count 미만인 단어는 제외하고 단어 dictionary 생성\n",
    "            if c < min_count:\n",
    "                continue\n",
    "            self.word2id[w] = wid\n",
    "            self.id2word[wid] = w\n",
    "            self.word_frequency[wid] = c\n",
    "            wid += 1\n",
    "\n",
    "        self.word_count = len(self.word2id)\n",
    "        print(\"Total embeddings: \" + str(len(self.word2id)))\n",
    "\n",
    "    def initTableDiscards(self):\n",
    "        # get a frequency table for sub-sampling. Note that the frequency is adjusted by\n",
    "        # sub-sampling tricks.\n",
    "        t = 0.0001\n",
    "        f = np.array(list(self.word_frequency.values())) / self.token_count\n",
    "        self.discards = np.sqrt(t / f) + (t / f)\n",
    "\n",
    "    def initTableNegatives(self):\n",
    "        # get a table for negative sampling, if word with index 2 appears twice, then 2 will be listed\n",
    "        # in the table twice.\n",
    "        pow_frequency = np.array(list(self.word_frequency.values())) ** 0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = np.round(ratio * DataReader.NEGATIVE_TABLE_SIZE)\n",
    "        for wid, c in enumerate(count):\n",
    "            self.negatives += [wid] * int(c)\n",
    "        self.negatives = np.array(self.negatives)\n",
    "        np.random.shuffle(self.negatives)\n",
    "        self.sampling_prob = ratio\n",
    "\n",
    "    def getNegatives(self, target, size):  # TODO check equality with target\n",
    "        if self.care_type == 0:\n",
    "            response = self.negatives[self.negpos:self.negpos + size]\n",
    "            self.negpos = (self.negpos + size) % len(self.negatives)\n",
    "            if len(response) != size:\n",
    "                return np.concatenate((response, self.negatives[0:self.negpos]))\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metapath2vec Dataset\n",
    "class Metapath2vecDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        # read in data, window_size and input filename\n",
    "        self.data = data\n",
    "        self.window_size = window_size # 타겟 단어 중심 몇 개의 단어를 볼 것인가\n",
    "        self.input_file = open(data.inputFileName)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of walks\n",
    "        return self.data.sentences_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the list of pairs (center, context, 5 negatives)\n",
    "        while True:\n",
    "            line = self.input_file.readline()\n",
    "            if not line:\n",
    "                self.input_file.seek(0, 0)\n",
    "                line = self.input_file.readline()\n",
    "\n",
    "            if len(line) > 1:\n",
    "                words = line.split()\n",
    "\n",
    "                if len(words) > 1:\n",
    "                    word_ids = [self.data.word2id[w] for w in words if\n",
    "                                w in self.data.word2id and np.random.rand() < self.data.discards[self.data.word2id[w]]]\n",
    "\n",
    "                    pair_catch = []\n",
    "                    for i, u in enumerate(word_ids):\n",
    "                        for j, v in enumerate(\n",
    "                                word_ids[max(i - self.window_size, 0):i + self.window_size]):\n",
    "                            assert u < self.data.word_count\n",
    "                            assert v < self.data.word_count\n",
    "                            if i == j:\n",
    "                                continue\n",
    "                            pair_catch.append((u, v, self.data.getNegatives(v,5)))\n",
    "                    return pair_catch\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batches):\n",
    "        all_u = [u for batch in batches for u, _, _ in batch if len(batch) > 0]\n",
    "        all_v = [v for batch in batches for _, v, _ in batch if len(batch) > 0]\n",
    "        all_neg_v = [neg_v for batch in batches for _, _, neg_v in batch if len(batch) > 0]\n",
    "\n",
    "        return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkipGram Model\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_dimension\n",
    "        init.uniform_(self.u_embeddings.weight.data, -initrange, initrange)\n",
    "        init.constant_(self.v_embeddings.weight.data, 0)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        emb_neg_v = self.v_embeddings(neg_v)\n",
    "\n",
    "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metapath2vec \n",
    "class Metapath2VecTrainer:\n",
    "    def __init__(self, path):\n",
    "        min_count, care_type = 0, 0\n",
    "        batch_size, iterations = 50, 1\n",
    "        window_size, dim, initial_lr = 10, 128, 0.025\n",
    "        num_workers = 1\n",
    "        \n",
    "        self.data = DataReader(path, min_count, care_type)\n",
    "        dataset = Metapath2vecDataset(self.data, window_size)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                                     shuffle=True, num_workers=num_workers, collate_fn=dataset.collate)\n",
    "        self.emb_size = len(self.data.word2id)\n",
    "        self.emb_dimension = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = iterations\n",
    "        self.initial_lr = initial_lr\n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        if self.use_cuda:\n",
    "            self.skip_gram_model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = optim.SparseAdam(list(self.skip_gram_model.parameters()), lr=self.initial_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
    "\n",
    "        for iteration in range(self.iterations):\n",
    "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
    "            running_loss = 0.0\n",
    "            for i, sample_batched in enumerate(tqdm(self.dataloader)):\n",
    "                if len(sample_batched[0]) > 1:\n",
    "                    pos_u = sample_batched[0].to(self.device)\n",
    "                    pos_v = sample_batched[1].to(self.device)\n",
    "                    neg_v = sample_batched[2].to(self.device)\n",
    "\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss = running_loss * 0.9 + loss.item() * 0.1\n",
    "                    if i > 0 and i % 50000 == 0:\n",
    "                        print(\" Loss: \" + str(running_loss))\n",
    "        \n",
    "        self.skip_gram_model.save_embedding(self.data.id2word, data_path+\"metapath_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Words...\n",
      "Total embeddings: 11437\n"
     ]
    }
   ],
   "source": [
    "m2v = Metapath2VecTrainer(os.path.join(data_path, \"metapath.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15539/2793940150.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.LongTensor(all_u), torch.LongTensor(all_v), torch.LongTensor(all_neg_v)\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|██████████| 1144/1144 [00:09<00:00, 124.68it/s]\n"
     ]
    }
   ],
   "source": [
    "m2v.train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python ('lightgcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
